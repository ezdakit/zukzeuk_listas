name: Scrape OTT Events and Generate M3U

on:
  schedule:
    # Se ejecuta cada hora a los 20 minutos
    - cron: '20 * * * *'
  workflow_dispatch:  # Permite ejecuci√≥n manual

jobs:
  scrape-and-generate:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml

      - name: Run scraping script with IPFS gateways
        id: scrape
        run: |
          cat > scrape_events.py << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          import json
          import re
          import os
          import hashlib
          from datetime import datetime, timedelta
          import time
          import random
          
          # Lista de gateways IPFS alternativos
          IPFS_GATEWAYS = [
              # Gateways p√∫blicos principales
              "https://ipfs.io",
              "https://gateway.ipfs.io",
              
              # Gateways de Cloudflare
              "https://cloudflare-ipfs.com",
              "https://cf-ipfs.com",
              
              # Gateways de Infura
              "https://ipfs.infura.io",
              "https://ipfs.infura-ipfs.io",
              
              # Gateways de Pinata
              "https://gateway.pinata.cloud",
              
              # Gateways de dweb
              "https://dweb.link",
              
              # Gateways de IPFS Public Gateway
              "https://gateway.ipfs.tech",
              "https://ipfs.eternum.io",
              
              # Otros gateways
              "https://ipfs.fleek.co",
              "https://ipfs.jes.xxx",
              "https://ipfs.runfission.com",
              "https://ipfs.eth.aragon.network",
              
              # Gateways regionales (pueden ser m√°s r√°pidos)
              "https://ipfs.decoo.io",
              "https://ipfs.anonymize.com",
              "https://ipfs.mrh.io",
          ]
          
          # Headers para evitar bloqueos
          HEADERS = {
              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
              'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
              'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
              'Accept-Encoding': 'gzip, deflate, br',
              'Connection': 'keep-alive',
              'Upgrade-Insecure-Requests': '1',
              'Cache-Control': 'max-age=0',
              'Referer': 'https://ipfs.io/',
          }
          
          def get_with_gateway_rotation(ipns_path, max_retries=3):
              """Obtiene contenido rotando entre diferentes gateways IPFS"""
              
              # URL base del IPNS
              ipns_hash = "k2k4r8oqlcjxsritt5mczkcn4mmvcmymbqw7113fz2flkrerfwfps004"
              
              # Mezclar gateways para rotaci√≥n
              shuffled_gateways = IPFS_GATEWAYS.copy()
              random.shuffle(shuffled_gateways)
              
              for attempt in range(max_retries):
                  for gateway in shuffled_gateways:
                      try:
                          # Construir URL completa
                          if ipns_path.startswith('/'):
                              ipns_path = ipns_path[1:]
                          
                          url = f"{gateway}/ipns/{ipns_hash}/{ipns_path}"
                          
                          print(f"Intento {attempt + 1}: Probando gateway {gateway}")
                          print(f"URL: {url}")
                          
                          response = requests.get(
                              url, 
                              headers=HEADERS, 
                              timeout=15,
                              allow_redirects=True
                          )
                          
                          if response.status_code == 200:
                              # Verificar que el contenido sea HTML v√°lido
                              if '<html' in response.text[:200].lower():
                                  print(f"‚úì √âxito con gateway: {gateway}")
                                  return response.text
                              else:
                                  print(f"‚úó Contenido no HTML de gateway: {gateway}")
                          else:
                              print(f"‚úó Error HTTP {response.status_code} de gateway: {gateway}")
                              
                      except requests.exceptions.Timeout:
                          print(f"‚úó Timeout con gateway: {gateway}")
                          
                      except requests.exceptions.ConnectionError:
                          print(f"‚úó Error de conexi√≥n con gateway: {gateway}")
                          
                      except requests.exceptions.RequestException as e:
                          print(f"‚úó Error con gateway {gateway}: {str(e)}")
                      
                      # Peque√±a pausa entre intentos del mismo gateway
                      time.sleep(0.5)
                  
                  # Pausa m√°s larga entre rondas de gateways
                  if attempt < max_retries - 1:
                      wait_time = 2 ** attempt
                      print(f"Esperando {wait_time} segundos antes de la siguiente ronda...")
                      time.sleep(wait_time)
              
              return None
          
          def test_gateways():
              """Test r√°pido de gateways para ver cu√°les funcionan"""
              working_gateways = []
              
              print("=== Probando gateways IPFS ===")
              
              # Test con un peque√±o recurso IPFS conocido
              test_hash = "QmT5NvUtoM5nWFfrQdVrFtvGfKFmG7AHE8P34isapyhCxX"  # Logo de IPFS
              
              for gateway in IPFS_GATEWAYS[:5]:  # Probar solo los primeros 5 para rapidez
                  try:
                      url = f"{gateway}/ipfs/{test_hash}"
                      response = requests.head(url, timeout=5, allow_redirects=True)
                      
                      if response.status_code in [200, 302]:
                          working_gateways.append(gateway)
                          print(f"‚úì {gateway} - FUNCIONA")
                      else:
                          print(f"‚úó {gateway} - Error {response.status_code}")
                  except:
                      print(f"‚úó {gateway} - INACCESIBLE")
              
              return working_gateways
          
          def extract_events(html_content):
              """Extrae eventos deportivos del HTML"""
              soup = BeautifulSoup(html_content, 'html.parser')
              events = []
              
              # Buscar la pesta√±a de agenda
              agenda_tab = soup.find('div', {'id': 'agendaTab'})
              if not agenda_tab:
                  print("No se encontr√≥ la pesta√±a de agenda")
                  return events
              
              # Encontrar todos los d√≠as con eventos
              # Basado en la estructura HTML proporcionada
              channel_groups = agenda_tab.find_all('div', {'class': 'channel-group'})
              
              for group in channel_groups:
                  # Extraer fecha del grupo
                  group_header = group.find('div', {'class': 'group-header'})
                  if group_header:
                      date_element = group_header.find('h3', {'class': 'group-title'})
                      if date_element:
                          date_str = date_element.text.strip()
                          
                          # Parsear fecha (ejemplo: "18-10" para 18 de octubre)
                          try:
                              # Intentar extraer d√≠a y mes
                              match = re.search(r'(\d{1,2})[-/](\d{1,2})', date_str)
                              if match:
                                  day = match.group(1).zfill(2)
                                  month = match.group(2).zfill(2)
                                  formatted_date = f"{day}-{month}"
                              else:
                                  # Si no encuentra patr√≥n, usar fecha actual
                                  today = datetime.now()
                                  formatted_date = today.strftime("%d-%m")
                          except:
                              today = datetime.now()
                              formatted_date = today.strftime("%d-%m")
                  
                  # Buscar tabla de eventos
                  events_table = group.find('table', {'class': 'events-table'})
                  if not events_table:
                      continue
                  
                  # Buscar filas de eventos
                  event_rows = events_table.find_all('tr', {'class': 'event-row'})
                  
                  for row in event_rows:
                      try:
                          # Extraer celdas
                          cells = row.find_all('td')
                          if len(cells) < 4:
                              continue
                          
                          # Hora (primera celda)
                          time_str = cells[0].text.strip()
                          
                          # Categor√≠a/Competici√≥n (segunda celda)
                          category = cells[1].text.strip()
                          
                          # Nombre del evento (tercera celda)
                          event_cell = cells[2]
                          
                          # Extraer nombres de equipos
                          team_containers = event_cell.find_all('div', {'class': 'team-container'})
                          if len(team_containers) >= 2:
                              team1 = team_containers[0].find('div', {'class': 'team-name'})
                              team2 = team_containers[1].find('div', {'class': 'team-name'})
                              
                              team1_name = team1.text.strip() if team1 else "Equipo 1"
                              team2_name = team2.text.strip() if team2 else "Equipo 2"
                              
                              event_name = f"{team1_name} vs {team2_name}"
                          else:
                              # Intentar extraer texto directamente
                              event_name = event_cell.text.strip()
                              # Limpiar espacios extra
                              event_name = re.sub(r'\s+', ' ', event_name)
                          
                          # Buscar canales Acestream en los detalles
                          channels = []
                          
                          # Buscar fila de detalles siguiente
                          detail_row = row.find_next_sibling('tr', {'class': 'event-detail'})
                          if detail_row:
                              # Buscar enlaces de stream
                              stream_links = detail_row.find_all('a', href=True)
                              
                              for link in stream_links:
                                  href = link.get('href', '')
                                  link_text = link.text.strip()
                                  
                                  # Buscar IDs de Acestream (40 caracteres hexadecimales)
                                  acestream_pattern = r'[a-fA-F0-9]{40}'
                                  matches = re.findall(acestream_pattern, href)
                                  
                                  if matches:
                                      acestream_id = matches[0]
                                      channel_name = link_text if link_text else f"Canal {len(channels)+1}"
                                      
                                      channels.append({
                                          'id': acestream_id,
                                          'name': channel_name
                                      })
                          
                          # Solo agregar eventos con canales Acestream
                          if channels:
                              # Limitar longitud del nombre del evento
                              if len(event_name) > 80:
                                  event_name = event_name[:77] + "..."
                              
                              events.append({
                                  'date': formatted_date,
                                  'time': time_str,
                                  'category': category,
                                  'name': event_name,
                                  'channels': channels
                              })
                              
                      except Exception as e:
                          print(f"Error procesando evento: {str(e)}")
                          continue
              
              return events
          
          def generate_m3u_content(events):
              """Genera contenido M3U a partir de los eventos"""
              lines = []
              
              # Encabezado M3U
              lines.append('#EXTM3U url-tvg="https://github.com/davidmuma/EPG_dobleM/raw/refs/heads/master/EPG_dobleM.xml,https://raw.githubusercontent.com/davidmuma/EPG_dobleM/refs/heads/master/EPG_dobleM.xml,https://epgshare01.online/epgshare01/epg_ripper_NL1.xml.gz" refresh="3600"')
              lines.append('#EXTVLCOPT:network-caching=1000')
              lines.append('')  # L√≠nea en blanco
              
              # Ordenar eventos por fecha y hora
              # Convertir horas a objetos datetime para ordenar correctamente
              def event_sort_key(event):
                  try:
                      # Parsear fecha y hora
                      date_parts = event['date'].split('-')
                      if len(date_parts) == 2:
                          day = int(date_parts[0])
                          month = int(date_parts[1])
                      else:
                          day = 1
                          month = 1
                      
                      time_parts = event['time'].split(':')
                      if len(time_parts) == 2:
                          hour = int(time_parts[0])
                          minute = int(time_parts[1])
                      else:
                          hour = 0
                          minute = 0
                      
                      # Usar a√±o actual
                      year = datetime.now().year
                      
                      return datetime(year, month, day, hour, minute)
                  except:
                      return datetime.now()
              
              events.sort(key=event_sort_key)
              
              # Generar entradas para cada evento-canal
              for event in events:
                  for channel in event['channels']:
                      # Formato: fecha + categor√≠a
                      group_title = f"{event['date']} {event['category']}"
                      
                      # Formato: hora + nombre evento + (primeros 3 chars del ID)
                      channel_display_name = f"{event['time']} {event['name']} ({channel['id'][:3]})"
                      
                      # URL Acestream
                      acestream_url = f"http://127.0.0.1:6878/ace/getstream?id={channel['id']}"
                      
                      # L√≠nea EXTINF
                      lines.append(f'#EXTINF:-1 group-title="{group_title}" tvg-name="{channel_display_name}",{channel_display_name}')
                      lines.append(acestream_url)
              
              return '\n'.join(lines)
          
          def main():
              # Primero probar gateways para ver cu√°les funcionan
              working_gateways = test_gateways()
              
              if working_gateways:
                  print(f"\nGateways funcionando: {len(working_gateways)}")
                  # Actualizar la lista de gateways con los que funcionan
                  global IPFS_GATEWAYS
                  IPFS_GATEWAYS = working_gateways + IPFS_GATEWAYS
                  IPFS_GATEWAYS = list(dict.fromkeys(IPFS_GATEWAYS))  # Eliminar duplicados
              
              print(f"\n=== Iniciando scraping con {len(IPFS_GATEWAYS)} gateways ===")
              
              # Ruta espec√≠fica con par√°metro tab=agenda
              ipns_path = "?tab=agenda"
              
              html_content = get_with_gateway_rotation(ipns_path)
              
              if not html_content:
                  print("ERROR: No se pudo obtener el contenido de ning√∫n gateway IPFS")
                  return
              
              print("\nContenido obtenido exitosamente, extrayendo eventos...")
              print(f"Tama√±o del contenido: {len(html_content)} caracteres")
              
              events = extract_events(html_content)
              
              print(f"\nResultados de extracci√≥n:")
              print(f"Eventos encontrados: {len(events)}")
              
              total_channels = sum(len(e['channels']) for e in events)
              print(f"Canales Acestream totales: {total_channels}")
              
              # Mostrar resumen por fecha
              dates_summary = {}
              for event in events:
                  date = event['date']
                  if date not in dates_summary:
                      dates_summary[date] = 0
                  dates_summary[date] += len(event['channels'])
              
              print("\nCanales por fecha:")
              for date, count in sorted(dates_summary.items()):
                  print(f"  {date}: {count} canales")
              
              if events:
                  m3u_content = generate_m3u_content(events)
                  
                  # Guardar archivo principal
                  with open('zz_eventos_ott.m3u', 'w', encoding='utf-8') as f:
                      f.write(m3u_content)
                  print(f"\n‚úì Archivo generado: zz_eventos_ott.m3u")
                  print(f"  L√≠neas: {m3u_content.count(chr(10)) + 1}")
                  
                  # Verificar si es diferente al anterior
                  history_dir = 'history'
                  os.makedirs(history_dir, exist_ok=True)
                  
                  # Calcular hash del contenido actual
                  current_hash = hashlib.md5(m3u_content.encode('utf-8')).hexdigest()
                  
                  # Buscar el √∫ltimo archivo en el hist√≥rico
                  history_files = sorted([f for f in os.listdir(history_dir) if f.endswith('.m3u')])
                  save_to_history = True
                  
                  if history_files:
                      last_file = os.path.join(history_dir, history_files[-1])
                      try:
                          with open(last_file, 'r', encoding='utf-8') as f:
                              last_content = f.read()
                          last_hash = hashlib.md5(last_content.encode('utf-8')).hexdigest()
                          
                          if current_hash == last_hash:
                              print("‚ÑπÔ∏è El contenido no ha cambiado, no se guardar√° en el hist√≥rico")
                              save_to_history = False
                          else:
                              print("‚úì Contenido diferente al anterior")
                      except:
                          print("‚ö†Ô∏è No se pudo leer el archivo hist√≥rico anterior")
                  
                  if save_to_history:
                      # Generar nombre con timestamp
                      timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                      history_file = os.path.join(history_dir, f'zz_eventos_ott_{timestamp}.m3u')
                      
                      with open(history_file, 'w', encoding='utf-8') as f:
                          f.write(m3u_content)
                      print(f"‚úì Archivo guardado en hist√≥rico: {history_file}")
                      
                      # Mantener solo los √∫ltimos 50 archivos
                      if len(history_files) >= 50:
                          files_to_delete = history_files[:-49]  # Mantener √∫ltimos 50
                          for file in files_to_delete:
                              try:
                                  os.remove(os.path.join(history_dir, file))
                                  print(f"  Eliminado archivo antiguo: {file}")
                              except:
                                  pass
                  
                  # Guardar eventos en JSON para depuraci√≥n
                  with open('events_debug.json', 'w', encoding='utf-8') as f:
                      json.dump(events, f, indent=2, ensure_ascii=False)
                  print("‚úì Archivo de depuraci√≥n generado: events_debug.json")
                  
                  # Crear salida para GitHub Actions
                  print(f"::set-output name=events_count::{len(events)}")
                  print(f"::set-output name=channels_count::{total_channels}")
                  print(f"::set-output name=success::true")
                  
              else:
                  print("\n‚úó No se encontraron eventos con canales Acestream")
                  print("::set-output name=success::false")
          
          if __name__ == '__main__':
              main()
          EOF
          
          # Ejecutar el script
          python scrape_events.py 2>&1 | tee scrape.log
          
          # Capturar estad√≠sticas del log
          EVENTS_COUNT=$(grep "Eventos encontrados:" scrape.log | grep -o '[0-9]*' || echo "0")
          CHANNELS_COUNT=$(grep "Canales Acestream totales:" scrape.log | grep -o '[0-9]*' || echo "0")
          SUCCESS=$(grep "::set-output name=success::" scrape.log | tail -1 | grep -o 'true\|false' || echo "false")
          
          echo "events_count=$EVENTS_COUNT" >> $GITHUB_OUTPUT
          echo "channels_count=$CHANNELS_COUNT" >> $GITHUB_OUTPUT
          echo "success=$SUCCESS" >> $GITHUB_OUTPUT

      - name: Mostrar resultados
        if: always()
        run: |
          echo "=== RESULTADOS DEL SCRAPING ==="
          echo "Eventos encontrados: ${{ steps.scrape.outputs.events_count }}"
          echo "Canales extra√≠dos: ${{ steps.scrape.outputs.channels_count }}"
          echo "√âxito: ${{ steps.scrape.outputs.success }}"
          echo ""
          echo "=== VISTA PREVIA DEL ARCHIVO M3U ==="
          if [ -f zz_eventos_ott.m3u ]; then
            echo "Primeras 15 l√≠neas:"
            head -15 zz_eventos_ott.m3u
            echo ""
            echo "√öltimas 5 l√≠neas:"
            tail -5 zz_eventos_ott.m3u
            echo ""
            echo "Total de l√≠neas: $(wc -l < zz_eventos_ott.m3u)"
          else
            echo "Archivo M3U no generado"
          fi
          echo ""
          echo "=== HIST√ìRICO ==="
          if [ -d history ]; then
            echo "Archivos en hist√≥rico: $(ls -1 history/*.m3u 2>/dev/null | wc -l)"
            echo "√öltimos 3 archivos:"
            ls -1t history/*.m3u 2>/dev/null | head -3
          fi

      - name: Subir archivos generados
        uses: actions/upload-artifact@v4
        with:
          name: ott-events-m3u
          path: |
            zz_eventos_ott.m3u
            history/
            events_debug.json
            scrape.log
          retention-days: 7

      - name: Commit y push de cambios
        if: github.ref == 'refs/heads/main' && steps.scrape.outputs.success == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Verificar si hay cambios
          git add zz_eventos_ott.m3u history/ events_debug.json
          
          if git diff --cached --quiet; then
            echo "No hay cambios para commitear"
          else
            TIMESTAMP=$(date '+%Y-%m-%d %H:%M')
            EVENTS_COUNT=${{ steps.scrape.outputs.events_count }}
            CHANNELS_COUNT=${{ steps.scrape.outputs.channels_count }}
            git commit -m "üì∫ Actualizaci√≥n eventos OTT ($TIMESTAMP) - $EVENTS_COUNT eventos, $CHANNELS_COUNT canales"
            git push
          fi

      - name: Notificar en caso de fallo
        if: failure()
        run: |
          echo "‚ùå El scraping ha fallado"
          echo "Revisa los logs para m√°s detalles"
          if [ -f scrape.log ]; then
            echo "=== √öLTIMAS 20 L√çNEAS DEL LOG ==="
            tail -20 scrape.log
          fi
